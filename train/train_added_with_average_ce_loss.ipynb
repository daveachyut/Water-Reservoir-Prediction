{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Loading and preprocessing train data...\n",
      "------------------------------\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './data/prepped/imgs_train.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-455ea8b39f6d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    335\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    336\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;34m'__main__'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 337\u001b[1;33m     \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    338\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-455ea8b39f6d>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    189\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    190\u001b[0m     \u001b[1;31m# Preprocess. Should write a func for this\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 191\u001b[1;33m     \u001b[0mimgs_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./data/prepped/imgs_train.npy'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    192\u001b[0m     \u001b[0mimgs_mask_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./data/prepped/imgs_mask_train.npy'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    193\u001b[0m     \u001b[0mimgs_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./data/prepped/imgs_val.npy'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\numpy\\lib\\npyio.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[0;32m    382\u001b[0m     \u001b[0mown_fid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    383\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbasestring\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 384\u001b[1;33m         \u001b[0mfid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    385\u001b[0m         \u001b[0mown_fid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    386\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mis_pathlib_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data/prepped/imgs_train.npy'"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"Train U-Net on reservoir images\n",
    "Example:\n",
    "    python3 train.py\n",
    "Notes:\n",
    "    Must be run from reservoir-id-cnn/train/\n",
    "    Prepped data should be in the: ./data/prepped/ directory\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import os\n",
    "from skimage import transform\n",
    "import numpy as np\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, concatenate, Conv2D, MaxPooling2D, Conv2DTranspose, UpSampling2D\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard, EarlyStopping\n",
    "from keras import backend as K\n",
    "from skimage import io\n",
    "\n",
    "\n",
    "K.set_image_data_format('channels_last')  # TF dimension ordering in this code\n",
    "\n",
    "OG_ROWS = 500\n",
    "OG_COLS = 500\n",
    "# Original image dimensions\n",
    "RESIZE_ROWS = 512\n",
    "RESIZE_COLS = 512\n",
    "# Resized dimensions for training/testing.\n",
    "NUM_BANDS = 6\n",
    "# Number of bands in image.\n",
    "SMOOTH = 1.\n",
    "# Smoothing factor for dice and jaccard coefficients\n",
    "PRED_THRESHOLD = 0.5\n",
    "# Prediction threshold. > PRED_THRESHOLD will be classified as res.\n",
    "\n",
    "\n",
    "def jaccard_coef(y_true, y_pred, smooth=SMOOTH):\n",
    "    \"\"\"Keras jaccard coefficient\n",
    "    @author: Vladimir Iglovikov\n",
    "    \"\"\"\n",
    "\n",
    "    intersection = K.sum(y_true * y_pred, axis=[0, -1, -2])\n",
    "    sum_ = K.sum(y_true + y_pred, axis=[0, -1, -2])\n",
    "\n",
    "    jac = (intersection + smooth) / (sum_ - intersection + smooth)\n",
    "\n",
    "    return K.mean(jac)\n",
    "\n",
    "\n",
    "def jaccard_distance_loss(y_true, y_pred, smooth=SMOOTH):\n",
    "    \"\"\"Keras jaccard loss function\n",
    "    Jaccard = (|X & Y|)/ (|X|+ |Y| - |X & Y|)\n",
    "            = sum(|A*B|)/(sum(|A|)+sum(|B|)-sum(|A*B|))\n",
    "    The jaccard distance loss is usefull for unbalanced datasets. This has been\n",
    "    shifted so it converges on 0 and is smoothed to avoid exploding or disapearing\n",
    "    gradient.\n",
    "    Ref: https://en.wikipedia.org/wiki/Jaccard_index\n",
    "    @url: https://gist.github.com/wassname/f1452b748efcbeb4cb9b1d059dce6f96\n",
    "    @author: wassname\n",
    "    \"\"\"\n",
    "\n",
    "    intersection = K.sum(K.abs(y_true * y_pred), axis=-1)\n",
    "    sum_ = K.sum(K.abs(y_true) + K.abs(y_pred), axis=-1)\n",
    "    jac = (intersection + smooth) / (sum_ - intersection + smooth)\n",
    "    return (1 - jac) * smooth\n",
    "\n",
    "\n",
    "def scale_image_tobyte(ar):\n",
    "    \"\"\"Scale larger data type array to byte\"\"\"\n",
    "    min_val = np.min(ar)\n",
    "    max_val = np.max(ar)\n",
    "    byte_ar = (np.round(255.0 * (ar - min_val) / (max_val - min_val))\n",
    "               .astype(np.uint8))\n",
    "    byte_ar[ar == 0] = 0\n",
    "\n",
    "    return(byte_ar)\n",
    "\n",
    "\n",
    "def dice_coef(y_true, y_pred, smooth=SMOOTH):\n",
    "    \"\"\"Keras implementation of Dice coefficient\"\"\"\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "\n",
    "    return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
    "\n",
    "\n",
    "def dice_coef_wgt(y_true, y_pred, smooth=SMOOTH):\n",
    "    \"\"\"Modified Dice, with Positive class given double weight\"\"\"\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    weights = K.cast(K.greater(y_true_f, 0.5), dtype='float32')\n",
    "    weights = weights + K.ones_like(weights)\n",
    "    intersection = K.sum(y_true_f * y_pred_f * weights)\n",
    "\n",
    "    return (2. * intersection + smooth) / (K.sum(y_true_f * weights) + K.sum(y_pred_f * weights) + smooth)\n",
    "\n",
    "\n",
    "def dice_coef_loss(y_true, y_pred):\n",
    "    \"\"\"Loss function is simply dice coefficient * -1\"\"\"\n",
    "    return -dice_coef(y_true, y_pred)\n",
    "\n",
    "def binary_crossentropy(y_true, y_pred):\n",
    "    result = []\n",
    "    for i in range(len(y_pred)):\n",
    "        y_pred[i] = [max(min(x, 1 - K.epsilon()), K.epsilon()) for x in y_pred[i]]\n",
    "        result.append(-np.mean([y_true[i][j] * math.log(y_pred[i][j]) + (1 - y_true[i][j]) * math.log(1 - y_pred[i][j]) for j in range(len(y_pred[i]))]))\n",
    "    return np.mean(result)\n",
    "\n",
    "\n",
    "def average_crossentropy(y_true, y_pred):\n",
    "    result = []\n",
    "    for i in range(len(y_pred)):\n",
    "        y_pred[i] = [max(min(x, 1 - K.epsilon()), K.epsilon()) for x in y_pred[i]]\n",
    "        result.append(sum([math.log(y_pred[i][j])*y_true[i][j] for j in range(len(y_pred[i]))]))\n",
    "    outcome=-(np.mean(result))\n",
    "    return outcome\n",
    "\n",
    "def get_unet(img_rows, img_cols, nbands):\n",
    "    \"\"\"U-Net Structure\n",
    "    @author: jocicmarko\n",
    "    @url: https://github.com/jocicmarko/ultrasound-nerve-segmentation\n",
    "    \"\"\"\n",
    "\n",
    "    inputs = Input((img_rows, img_cols, nbands))\n",
    "    conv1 = Conv2D(32, (3, 3), activation='relu', padding='same')(inputs)\n",
    "    conv1 = Conv2D(32, (3, 3), activation='relu', padding='same')(conv1)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "\n",
    "    conv2 = Conv2D(64, (3, 3), activation='relu', padding='same')(pool1)\n",
    "    conv2 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv2)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "\n",
    "    conv3 = Conv2D(128, (3, 3), activation='relu', padding='same')(pool2)\n",
    "    conv3 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv3)\n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "\n",
    "    conv4 = Conv2D(256, (3, 3), activation='relu', padding='same')(pool3)\n",
    "    conv4 = Conv2D(256, (3, 3), activation='relu', padding='same')(conv4)\n",
    "    pool4 = MaxPooling2D(pool_size=(2, 2))(conv4)\n",
    "\n",
    "    conv5 = Conv2D(512, (3, 3), activation='relu', padding='same')(pool4)\n",
    "    conv5 = Conv2D(512, (3, 3), activation='relu', padding='same')(conv5)\n",
    "\n",
    "    up6 = concatenate([Conv2DTranspose(256, (2, 2), strides=(2, 2),\n",
    "                                       padding='same')(conv5), conv4], axis=3)\n",
    "    conv6 = Conv2D(256, (3, 3), activation='relu', padding='same')(up6)\n",
    "    conv6 = Conv2D(256, (3, 3), activation='relu', padding='same')(conv6)\n",
    "\n",
    "    up7 = concatenate([Conv2DTranspose(128, (2, 2), strides=(2, 2),\n",
    "                                       padding='same')(conv6), conv3], axis=3)\n",
    "    conv7 = Conv2D(128, (3, 3), activation='relu', padding='same')(up7)\n",
    "    conv7 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv7)\n",
    "\n",
    "    up8 = concatenate([Conv2DTranspose(64, (2, 2), strides=(2, 2),\n",
    "                                       padding='same')(conv7), conv2], axis=3)\n",
    "    conv8 = Conv2D(64, (3, 3), activation='relu', padding='same')(up8)\n",
    "    conv8 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv8)\n",
    "\n",
    "    up9 = concatenate([Conv2DTranspose(32, (2, 2), strides=(2, 2),\n",
    "                                       padding='same')(conv8), conv1], axis=3)\n",
    "    conv9 = Conv2D(32, (3, 3), activation='relu', padding='same')(up9)\n",
    "    conv9 = Conv2D(32, (3, 3), activation='relu', padding='same')(conv9)\n",
    "\n",
    "    conv10 = Conv2D(1, (1, 1), activation='sigmoid')(conv9)\n",
    "\n",
    "    model = Model(inputs=[inputs], outputs=[conv10])\n",
    "\n",
    "    model.compile(optimizer=Adam(lr=5e-5),\n",
    "                  loss=dice_coef_loss,\n",
    "                  metrics=[jaccard_coef, dice_coef,\n",
    "                           'accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def resize_imgs(imgs, nbands):\n",
    "    \"\"\"Resize numpy array of images\"\"\"\n",
    "    imgs_p = np.ndarray((imgs.shape[0], RESIZE_ROWS, RESIZE_COLS, nbands))\n",
    "\n",
    "    for i in range(imgs.shape[0]):\n",
    "        imgs_p[i] = transform.resize(imgs[i],\n",
    "                                     (RESIZE_ROWS, RESIZE_COLS, nbands),\n",
    "                                     preserve_range = True)\n",
    "\n",
    "    return imgs_p\n",
    "\n",
    "\n",
    "def train():\n",
    "    \"\"\"Master function for training\"\"\"\n",
    "    print('-'*30)\n",
    "    print('Loading and preprocessing train data...')\n",
    "    print('-'*30)\n",
    "\n",
    "    # Preprocess. Should write a func for this\n",
    "    imgs_train = np.load('./data/prepped/imgs_train.npy')\n",
    "    imgs_mask_train = np.load('./data/prepped/imgs_mask_train.npy')\n",
    "    imgs_val = np.load('./data/prepped/imgs_val.npy')\n",
    "    imgs_mask_val = np.load('./data/prepped/imgs_mask_val.npy')\n",
    "\n",
    "    imgs_train = resize_imgs(imgs_train, NUM_BANDS)\n",
    "    imgs_mask_train = resize_imgs(imgs_mask_train, 1)\n",
    "    imgs_val = resize_imgs(imgs_val, NUM_BANDS)\n",
    "    imgs_mask_val = resize_imgs(imgs_mask_val, 1)\n",
    "\n",
    "    imgs_train = imgs_train.astype('float32')\n",
    "    imgs_mask_train = imgs_mask_train.astype('float32')\n",
    "    imgs_val = imgs_val.astype('float32')\n",
    "    imgs_mask_val = imgs_mask_val.astype('float32')\n",
    "\n",
    "    mean = np.mean(imgs_train, axis=(0,1,2))  # mean for data centering\n",
    "    std = np.std(imgs_train, axis=(0,1,2))  # std for data normalization\n",
    "\n",
    "    # Save the mean and std values\n",
    "    np.save('mean_std.npy', np.vstack((mean, std)))\n",
    "\n",
    "    imgs_train -= mean\n",
    "    imgs_train /= std\n",
    "    imgs_val -= mean\n",
    "    imgs_val /= std\n",
    "\n",
    "    imgs_mask_train /= 255.  # scale masks to [0, 1]\n",
    "    imgs_mask_train[imgs_mask_train >= 0.5] = 1\n",
    "    imgs_mask_train[imgs_mask_train < 0.5] = 0\n",
    "    imgs_mask_val /= 255.  # scale masks to [0, 1]\n",
    "    imgs_mask_val[imgs_mask_val >= 0.5] = 1\n",
    "    imgs_mask_val[imgs_mask_val < 0.5] = 0\n",
    "\n",
    "    print('-'*30)\n",
    "    print('Creating and compiling model...')\n",
    "    print('-'*30)\n",
    "    model = get_unet(RESIZE_ROWS, RESIZE_COLS, NUM_BANDS)\n",
    "\n",
    "    # Setup callbacks\n",
    "    model_checkpoint = ModelCheckpoint('weights.h5', monitor='val_loss',\n",
    "                                       save_best_only=True)\n",
    "    tensorboard = TensorBoard(log_dir='./logs', histogram_freq=0,\n",
    "                              write_images=True)\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=8,\n",
    "                                   verbose=0, mode='auto')\n",
    "\n",
    "\n",
    "    print('-'*30)\n",
    "    print('Fitting model...')\n",
    "    print('-'*30)\n",
    "    \n",
    "    #u_net_estimator = KerasClassifier(build_fn= simple_model, epochs=100, batch_size=10, verbose=0)\n",
    "\n",
    "    #boosted_u_net = AdaBoostClassifier(base_estimator= u_net_estimator)\n",
    "    #boosted_u_net.fit(imgs_train, imgs_mask_train.values.ravel())# scale your training data \n",
    "    \n",
    "    \n",
    "    model.fit(imgs_train, imgs_mask_train, batch_size=8, epochs=500,\n",
    "              verbose=1, shuffle=True, class_weight={1:10},\n",
    "              validation_data=(imgs_val, imgs_mask_val),\n",
    "              callbacks=[model_checkpoint, tensorboard, early_stopping])\n",
    "\n",
    "    print('-'*30)\n",
    "    print('Loading and preprocessing test data...')\n",
    "    print('-'*30)\n",
    "    imgs_test = np.load('./data/prepped/imgs_test.npy')\n",
    "    imgs_mask_test = np.load('./data/prepped/imgs_mask_test.npy')\n",
    "\n",
    "    imgs_test = resize_imgs(imgs_test, NUM_BANDS)\n",
    "\n",
    "    imgs_test = imgs_test.astype('float32')\n",
    "    imgs_test -= mean\n",
    "    imgs_test /= std\n",
    "\n",
    "    print('-'*30)\n",
    "    print('Loading saved weights...')\n",
    "    print('-'*30)\n",
    "    model.load_weights('weights.h5')\n",
    "\n",
    "    print('-'*30)\n",
    "    print('Predicting masks on test data...')\n",
    "    print('-'*30)\n",
    "    #pred_test_masks = boosted_u_net.predict(imgs_test, batch_size=8, verbose=1)\n",
    "    model.predict(imgs_test, batch_size=8, verbose=1)\n",
    "\n",
    "    # Save predicted masks\n",
    "    predict_dir = './data/predict/'\n",
    "    if not os.path.isdir(predict_dir):\n",
    "        os.makedirs(predict_dir)\n",
    "\n",
    "    np.save('{}pred_test_masks.npy'.format(predict_dir), pred_test_masks)\n",
    "    test_img_names = open('./data/prepped/test_names.csv').read().splitlines()\n",
    "\n",
    "    # For calculating total error metrics\n",
    "    total_res_pixels = 0\n",
    "    total_true_positives = 0\n",
    "    total_false_positives = 0\n",
    "    for i in range(pred_test_masks.shape[0]):\n",
    "        pred_mask = pred_test_masks[i]\n",
    "        true_mask = imgs_mask_test[i]\n",
    "\n",
    "        # Get ndwi as byte\n",
    "        ndwi_img = imgs_test[i,:,:,4]\n",
    "        ndwi_img = transform.resize(ndwi_img,\n",
    "                                    (OG_ROWS, OG_COLS),\n",
    "                                    preserve_range = True)\n",
    "        ndwi_img = scale_image_tobyte(ndwi_img)\n",
    "        ndwi_img = ndwi_img.astype('uint8')\n",
    "\n",
    "        print(np.min(pred_mask), np.max(pred_mask))\n",
    "        pred_mask = transform.resize(pred_mask,\n",
    "                                     (OG_ROWS, OG_COLS),\n",
    "                                     preserve_range = True)\n",
    "        pred_mask = (pred_mask[:, :, 0] * 255.).astype(np.uint8)\n",
    "\n",
    "        # Save predicted masks\n",
    "        pred_mask_filename = test_img_names[i].replace('og.tif', 'predmask.png')\n",
    "        io.imsave('{}{}'.format(predict_dir, pred_mask_filename), pred_mask)\n",
    "\n",
    "        # Save NDWI, predicted mask, and actual masks side by side\n",
    "        compare_filename = test_img_names[i].replace('og.tif', 'results.png')\n",
    "        compare_im = 255 * np.ones((OG_ROWS, OG_COLS * 3 + 20), dtype=np.uint8)\n",
    "        compare_im[0:OG_ROWS, 0:OG_COLS] = ndwi_img\n",
    "        compare_im[0:OG_ROWS, (OG_COLS + 10):(OG_COLS * 2 + 10)] = true_mask\n",
    "        compare_im[0:OG_ROWS, (OG_COLS * 2 + 20):] = pred_mask\n",
    "        io.imsave('{}{}'.format(predict_dir, compare_filename), compare_im)\n",
    "\n",
    "        # Calculate basic error\n",
    "        pred_mask = 255*(pred_mask > (PRED_THRESHOLD*255)) # move this to earlier\n",
    "        total_res_pixels += np.sum(true_mask == 255)\n",
    "        total_true_positives += np.sum((true_mask == 255) * (pred_mask == 255))\n",
    "        total_false_positives += np.sum((true_mask == 0) * (pred_mask == 255))\n",
    "\n",
    "        i += 1\n",
    "\n",
    "    print('Total Res Pixels: {}'.format(total_res_pixels))\n",
    "    print('Total True Pos: {} ({})'\n",
    "          .format(total_true_positives,\n",
    "                  total_true_positives/total_res_pixels))\n",
    "    print('Total False Pos: {} ({})'\n",
    "          .format(total_false_positives,\n",
    "                  total_false_positives/total_res_pixels))\n",
    "\n",
    "    return\n",
    "\n",
    "if __name__=='__main__':\n",
    "    train()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
